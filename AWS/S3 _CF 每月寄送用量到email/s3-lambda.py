import boto3
import datetime
from botocore.config import Config

CFG = Config(retries={'max_attempts': 5}, connect_timeout=2, read_timeout=8)

sns = boto3.client('sns', region_name='us-east-1', config=CFG)   # SNS æ”¾ä½ æƒ³ç”¨çš„å€
s3 = boto3.client('s3', config=CFG)

TOPIC_ARN = 'arn:aws:sns:us-east-1:your-aws-account-id:your-topic-name'  # æ›æˆä½ çš„SNS TOPIC ARN

# å¿«å–ï¼šä¸åŒå€åŸŸçš„ CloudWatch client
cw_clients = {}


def cloudwatch_in(region: str):
    if region not in cw_clients:
        cw_clients[region] = boto3.client(
            'cloudwatch', region_name=region, config=CFG)
    return cw_clients[region]


def get_bucket_region(bucket: str) -> str:
    # get_bucket_location å› None ä»£è¡¨ us-east-1
    r = s3.get_bucket_location(Bucket=bucket).get('LocationConstraint')
    return r or 'us-east-1'


def list_storage_types_for_bucket(cw, bucket: str):
    """åœ¨æ­£ç¢ºå€åŸŸçš„ CloudWatch åˆ—å‡ºè©² bucket çš„ StorageType æ¸…å–®"""
    paginator = cw.get_paginator('list_metrics')
    types = set()
    for page in paginator.paginate(
        Namespace='AWS/S3',
        MetricName='BucketSizeBytes',
        Dimensions=[{'Name': 'BucketName', 'Value': bucket}]
    ):
        for m in page.get('Metrics', []):
            for d in m.get('Dimensions', []):
                if d['Name'] == 'StorageType':
                    types.add(d['Value'])
    return sorted(types)


def get_latest_bytes(cw, bucket: str, storage_type: str):
    """åˆ°æ­£ç¢ºå€åŸŸæŠ“è©² StorageType æœ€æ–°ä¸€å¤©çš„ BucketSizeBytesï¼ˆbytesï¼‰"""
    end = datetime.datetime.utcnow()
    start = end - datetime.timedelta(days=3)  # S3 å„²å­˜é‡æ¯æ—¥æ›´æ–°ï¼Œå›æº¯å¹¾å¤©é¿å…å»¶é²
    resp = cw.get_metric_statistics(
        Namespace='AWS/S3',
        MetricName='BucketSizeBytes',
        Dimensions=[
            {'Name': 'BucketName',  'Value': bucket},
            {'Name': 'StorageType', 'Value': storage_type}
        ],
        StartTime=start, EndTime=end, Period=86400, Statistics=['Average']
    )
    dps = sorted(resp.get('Datapoints', []), key=lambda x: x['Timestamp'])
    return dps[-1]['Average'] if dps else None  # bytes


def get_bucket_size_kb_total(bucket: str, region: str):
    cw = cloudwatch_in(region)
    stypes = list_storage_types_for_bucket(cw, bucket)
    if not stypes:
        return None  # å¤šåŠæ˜¯ç©º bucket æˆ–æŒ‡æ¨™å°šæœªç”¢ç”Ÿ
    total = 0
    has_data = False
    for st in stypes:
        val = get_latest_bytes(cw, bucket, st)
        if val is not None:
            total += val
            has_data = True
    return (total / 1024.0) if has_data else None  # KB


def lambda_handler(event, context):
    buckets = [b['Name'] for b in s3.list_buckets()['Buckets']]
    rows = []
    for b in buckets:
        region = get_bucket_region(b)
        kb = get_bucket_size_kb_total(b, region)
        rows.append((b, region, kb))

    # ä¾ç”¨é‡æ’åºï¼ˆN/A æ”¾æœ€å¾Œï¼‰
    rows.sort(key=lambda x: (-1 if x[2] is None else x[2]), reverse=True)

    # æ˜ç¢ºé€è¡Œåˆ—å°ï¼Œæ¯å€‹ bucket ç¨ç«‹ä¸€è¡Œ
    report_lines = []
    report_lines.append("ğŸ“¦ S3 Storage Report (All buckets, KB)")
    report_lines.append(f"Total buckets: {len(rows)}")
    report_lines.append("")  # ç©ºè¡Œ
    for b, region, kb in rows:
        report_lines.append(
            f"{b} [{region}]: {'N/A' if kb is None else f'{kb:,.2f} KB'}")

    # ä»¥ \n é€£æ¥ï¼Œç¢ºä¿æ¯è¡Œåˆ†é–‹
    msg = "```\n" + "\n\n".join(report_lines) + "\n```"

    sns.publish(
        TopicArn=TOPIC_ARN,
        Subject='S3 Monthly Usage Report (KB)',
        Message=msg
    )

    return {"buckets": len(buckets)}
